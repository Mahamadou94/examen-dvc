{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as  pd           \n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['date', 'ave_flot_air_flow', 'ave_flot_level', 'iron_feed',\n",
       "       'starch_flow', 'amina_flow', 'ore_pulp_flow', 'ore_pulp_pH',\n",
       "       'ore_pulp_density', 'silica_concentrate'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"/home/ubuntu/Data_scientest/examen-dvc/data/raw_data/raw.csv\")\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1817 entries, 0 to 1816\n",
      "Data columns (total 10 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   date                1817 non-null   object \n",
      " 1   ave_flot_air_flow   1817 non-null   float64\n",
      " 2   ave_flot_level      1817 non-null   float64\n",
      " 3   iron_feed           1817 non-null   float64\n",
      " 4   starch_flow         1817 non-null   float64\n",
      " 5   amina_flow          1817 non-null   float64\n",
      " 6   ore_pulp_flow       1817 non-null   float64\n",
      " 7   ore_pulp_pH         1817 non-null   float64\n",
      " 8   ore_pulp_density    1817 non-null   float64\n",
      " 9   silica_concentrate  1817 non-null   float64\n",
      "dtypes: float64(9), object(1)\n",
      "memory usage: 142.1+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1817"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.iloc[ :, 0:8]\n",
    "Y = data.loc[:,[\"silica_concentrate\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1817 \n",
      "\n",
      "1817\n"
     ]
    }
   ],
   "source": [
    "print(len(X), \"\\n\")\n",
    "print(len(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def load_and_split_data(file_path):\n",
    "    data = pd.read_csv(file_path)\n",
    "    X = data.iloc[:, :-1]  \n",
    "    y = data.iloc[:, -1]  \n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Sauvegarde des datasets\n",
    "    X_train.to_csv('/home/ubuntu/Data_scientest/examen-dvc/data/processed_data/X_train.csv', index=False)\n",
    "    X_test.to_csv('/home/ubuntu/Data_scientest/examen-dvc/data/processed_data/X_test.csv', index=False)\n",
    "    y_train.to_csv('/home/ubuntu/Data_scientest/examen-dvc/data/processed_data/y_train.csv', index=False)\n",
    "    y_test.to_csv('/home/ubuntu/Data_scientest/examen-dvc/data/processed_data/y_test.csv', index=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    load_and_split_data('/home/ubuntu/Data_scientest/examen-dvc/data/raw_data/raw.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1453, 1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train = pd.read_csv(\"/home/ubuntu/Data_scientest/examen-dvc/data/processed_data/y_train.csv\")\n",
    "Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1453, 9)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = pd.read_csv(\"/home/ubuntu/Data_scientest/examen-dvc/data/processed_data/X_train.csv\")\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(364, 1)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test = pd.read_csv(\"/home/ubuntu/Data_scientest/examen-dvc/data/processed_data/y_test.csv\")\n",
    "Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(364, 9)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = pd.read_csv(\"/home/ubuntu/Data_scientest/examen-dvc/data/processed_data/X_test.csv\")\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ave_flot_air_flow  ave_flot_level  iron_feed  starch_flow  amina_flow  \\\n",
      "0         283.249788      371.504709      52.61  1377.723930  532.349411   \n",
      "1         283.484967      450.359552      52.25  4751.441833  565.239811   \n",
      "2         283.315531      566.688556      49.35  2848.397889  357.669117   \n",
      "3         283.283699      317.947923      52.61  3906.382833  503.500844   \n",
      "4         300.013755      499.279128      57.46  3099.747623  559.552975   \n",
      "\n",
      "   ore_pulp_flow  ore_pulp_pH  ore_pulp_density  year  month  day  hour  \n",
      "0     399.775294     9.980124          1.754136  2017      5   10     5  \n",
      "1     399.653050    10.384877          1.687133  2017      7    7     1  \n",
      "2     399.706589    10.149492          1.624664  2017      7   13     5  \n",
      "3     401.137750     9.738740          1.729456  2017      5   11     6  \n",
      "4     398.994144    10.236663          1.719852  2017      7   26     8  \n",
      "   ave_flot_air_flow  ave_flot_level  iron_feed  starch_flow  amina_flow  \\\n",
      "0         299.438455      567.409196      53.79  1537.443864  563.536911   \n",
      "1         300.126757      567.585000      64.03  3034.803153  387.048811   \n",
      "2         299.738475      417.715983      51.70  1820.073286  573.697128   \n",
      "3         283.269159      399.781715      64.03  3229.668361  526.246483   \n",
      "4         299.957979      497.800815      64.03  2743.199468  494.364989   \n",
      "\n",
      "   ore_pulp_flow  ore_pulp_pH  ore_pulp_density  year  month  day  hour  \n",
      "0     399.236850    10.008087          1.705529  2017      7   11    10  \n",
      "1     399.842656    10.037000          1.674957  2017      5   30    15  \n",
      "2     399.930439     9.861258          1.778173  2017      5    2     1  \n",
      "3     399.903383     9.850168          1.715880  2017      5   17    18  \n",
      "4     399.061072     9.901435          1.711293  2017      5   26     8  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Fonction pour charger et préparer les données\n",
    "def prepare_data(file_path):\n",
    "    data = pd.read_csv(file_path)\n",
    "\n",
    "    # Assurer que la colonne de date est traitée comme un type datetime\n",
    "    data['date'] = pd.to_datetime(data['date'])\n",
    "\n",
    "    # Extraire les caractéristiques de la date\n",
    "    data['year'] = data['date'].dt.year\n",
    "    data['month'] = data['date'].dt.month\n",
    "    data['day'] = data['date'].dt.day\n",
    "    data['hour'] = data['date'].dt.hour\n",
    "\n",
    "    # Supprimer la colonne originale de date\n",
    "    data.drop('date', axis=1, inplace=True)\n",
    "\n",
    "    return data\n",
    "\n",
    "# Préparer X_train et X_test\n",
    "X_train = prepare_data('/home/ubuntu/Data_scientest/examen-dvc/data/processed_data/X_train.csv')\n",
    "X_test = prepare_data('/home/ubuntu/Data_scientest/examen-dvc/data/processed_data/X_test.csv')\n",
    "\n",
    "# Vérifier les données\n",
    "print(X_train.head())\n",
    "print(X_test.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ave_flot_air_flow  ave_flot_level  iron_feed  starch_flow  amina_flow  \\\n",
      "0           0.031010        0.153083   0.412316     0.189369    0.585209   \n",
      "1           0.043608        0.339496   0.396252     0.843892    0.668547   \n",
      "2           0.034531        0.614498   0.266845     0.474689    0.142599   \n",
      "3           0.032826        0.026474   0.412316     0.679945    0.512111   \n",
      "4           0.929028        0.455142   0.628737     0.523453    0.654138   \n",
      "\n",
      "   ore_pulp_flow  ore_pulp_pH  ore_pulp_density  year  month       day  \\\n",
      "0       0.503025     0.600731          0.826511   0.0    0.2  0.300000   \n",
      "1       0.485170     0.808431          0.559285   0.0    0.6  0.200000   \n",
      "2       0.492990     0.687643          0.310139   0.0    0.6  0.400000   \n",
      "3       0.702032     0.476864          0.728082   0.0    0.2  0.333333   \n",
      "4       0.388927     0.732375          0.689778   0.0    0.6  0.833333   \n",
      "\n",
      "       hour  \n",
      "0  0.217391  \n",
      "1  0.043478  \n",
      "2  0.217391  \n",
      "3  0.260870  \n",
      "4  0.347826  \n",
      "   ave_flot_air_flow  ave_flot_level  iron_feed  starch_flow  amina_flow  \\\n",
      "0           0.898210        0.616202   0.464971     0.220356    0.664232   \n",
      "1           0.935081        0.616617   0.921910     0.510853    0.217042   \n",
      "2           0.914281        0.262326   0.371709     0.275188    0.689977   \n",
      "3           0.032047        0.219930   0.921910     0.548658    0.569745   \n",
      "4           0.926040        0.451647   0.921910     0.454280    0.488963   \n",
      "\n",
      "   ore_pulp_flow  ore_pulp_pH  ore_pulp_density  year  month       day  \\\n",
      "0       0.424377     0.615081          0.632654   0.0    0.6  0.333333   \n",
      "1       0.512864     0.629918          0.510726   0.0    0.2  0.966667   \n",
      "2       0.525686     0.539735          0.922378   0.0    0.2  0.033333   \n",
      "3       0.521734     0.534044          0.673935   0.0    0.2  0.533333   \n",
      "4       0.398702     0.560352          0.655642   0.0    0.2  0.833333   \n",
      "\n",
      "       hour  \n",
      "0  0.434783  \n",
      "1  0.652174  \n",
      "2  0.043478  \n",
      "3  0.782609  \n",
      "4  0.347826  \n"
     ]
    }
   ],
   "source": [
    "# Initialiser le MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Ajuster et transformer les données d'entraînement\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Transformer les données de test\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convertir les données transformées en DataFrame pour une utilisation ultérieure\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns)\n",
    "\n",
    "# Afficher les données transformées\n",
    "print(X_train_scaled.head())\n",
    "print(X_test_scaled.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled.to_csv('/home/ubuntu/Data_scientest/examen-dvc/data/processed_data/X_train_scaled.csv', index=False)\n",
    "X_test_scaled.to_csv('/home/ubuntu/Data_scientest/examen-dvc/data/processed_data/X_test_scaled.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ave_flot_air_flow  ave_flot_level  iron_feed  starch_flow  amina_flow  \\\n",
      "0           0.031010        0.153083   0.412316     0.189369    0.585209   \n",
      "1           0.043608        0.339496   0.396252     0.843892    0.668547   \n",
      "2           0.034531        0.614498   0.266845     0.474689    0.142599   \n",
      "3           0.032826        0.026474   0.412316     0.679945    0.512111   \n",
      "4           0.929028        0.455142   0.628737     0.523453    0.654138   \n",
      "\n",
      "   ore_pulp_flow  ore_pulp_pH  ore_pulp_density  year  month       day  \\\n",
      "0       0.503025     0.600731          0.826511   0.0    0.2  0.300000   \n",
      "1       0.485170     0.808431          0.559285   0.0    0.6  0.200000   \n",
      "2       0.492990     0.687643          0.310139   0.0    0.6  0.400000   \n",
      "3       0.702032     0.476864          0.728082   0.0    0.2  0.333333   \n",
      "4       0.388927     0.732375          0.689778   0.0    0.6  0.833333   \n",
      "\n",
      "       hour  \n",
      "0  0.217391  \n",
      "1  0.043478  \n",
      "2  0.217391  \n",
      "3  0.260870  \n",
      "4  0.347826  \n",
      "   ave_flot_air_flow  ave_flot_level  iron_feed  starch_flow  amina_flow  \\\n",
      "0           0.898210        0.616202   0.464971     0.220356    0.664232   \n",
      "1           0.935081        0.616617   0.921910     0.510853    0.217042   \n",
      "2           0.914281        0.262326   0.371709     0.275188    0.689977   \n",
      "3           0.032047        0.219930   0.921910     0.548658    0.569745   \n",
      "4           0.926040        0.451647   0.921910     0.454280    0.488963   \n",
      "\n",
      "   ore_pulp_flow  ore_pulp_pH  ore_pulp_density  year  month       day  \\\n",
      "0       0.424377     0.615081          0.632654   0.0    0.6  0.333333   \n",
      "1       0.512864     0.629918          0.510726   0.0    0.2  0.966667   \n",
      "2       0.525686     0.539735          0.922378   0.0    0.2  0.033333   \n",
      "3       0.521734     0.534044          0.673935   0.0    0.2  0.533333   \n",
      "4       0.398702     0.560352          0.655642   0.0    0.2  0.833333   \n",
      "\n",
      "       hour  \n",
      "0  0.434783  \n",
      "1  0.652174  \n",
      "2  0.043478  \n",
      "3  0.782609  \n",
      "4  0.347826  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def prepare_data(file_path):\n",
    "    \"\"\"\n",
    "    Charge les données à partir d'un chemin de fichier, convertit les colonnes de date en datetime,\n",
    "    extrait les caractéristiques temporelles et supprime la colonne de date originale.\n",
    "    \n",
    "    Args:\n",
    "    file_path (str): Chemin complet du fichier CSV à charger.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame avec les colonnes de date traitées.\n",
    "    \"\"\"\n",
    "    data = pd.read_csv(file_path)\n",
    "    data['date'] = pd.to_datetime(data['date'])  # Conversion de la colonne 'date' en datetime\n",
    "    data['year'] = data['date'].dt.year\n",
    "    data['month'] = data['date'].dt.month\n",
    "    data['day'] = data['date'].dt.day\n",
    "    data['hour'] = data['date'].dt.hour\n",
    "    data.drop('date', axis=1, inplace=True)  # Suppression de la colonne 'date'\n",
    "    return data\n",
    "\n",
    "def normalize_data(X_train, X_test):\n",
    "    \"\"\"\n",
    "    Normalise les ensembles de données d'entraînement et de test en utilisant MinMaxScaler.\n",
    "\n",
    "    Args:\n",
    "    X_train (pd.DataFrame): Données d'entraînement à normaliser.\n",
    "    X_test (pd.DataFrame): Données de test à normaliser.\n",
    "\n",
    "    Returns:\n",
    "    tuple: contient les DataFrames des données d'entraînement et de test normalisées.\n",
    "    \"\"\"\n",
    "    scaler = MinMaxScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
    "    X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns)\n",
    "    return X_train_scaled, X_test_scaled\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Chemins des fichiers\n",
    "    train_path = '/home/ubuntu/Data_scientest/examen-dvc/data/processed_data/X_train.csv'\n",
    "    test_path = '/home/ubuntu/Data_scientest/examen-dvc/data/processed_data/X_test.csv'\n",
    "\n",
    "    # Préparation des données\n",
    "    X_train = prepare_data(train_path)\n",
    "    X_test = prepare_data(test_path)\n",
    "\n",
    "    # Normalisation des données\n",
    "    X_train_scaled, X_test_scaled = normalize_data(X_train, X_test)\n",
    "\n",
    "    # Sauvegarde des données normalisées\n",
    "    X_train_scaled.to_csv('/home/ubuntu/Data_scientest/examen-dvc/data/processed_data/X_train_scaled.csv', index=False)\n",
    "    X_test_scaled.to_csv('/home/ubuntu/Data_scientest/examen-dvc/data/processed_data/X_test_scaled.csv', index=False)\n",
    "\n",
    "    # Exemple d'affichage des données normalisées\n",
    "    print(X_train_scaled.head())\n",
    "    print(X_test_scaled.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1453"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1453"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import joblib\n",
    "\n",
    "def perform_grid_search():\n",
    "    # Charger les données d'entraînement\n",
    "    X_train = pd.read_csv('/home/ubuntu/Data_scientest/examen-dvc/data/processed_data/X_train_scaled.csv')\n",
    "    y_train = pd.read_csv('/home/ubuntu/Data_scientest/examen-dvc/data/processed_data/y_train.csv')\n",
    "\n",
    "    # Définir le modèle et les paramètres pour GridSearch\n",
    "    model = RandomForestRegressor(random_state=42)\n",
    "    param_grid = {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [10, 20, 30]\n",
    "    }\n",
    "    \n",
    "    # Créer l'objet GridSearchCV\n",
    "    grid_search = GridSearchCV(model, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "    \n",
    "    # Exécuter le GridSearch\n",
    "    grid_search.fit(X_train, y_train.values.ravel())\n",
    "    \n",
    "    # Sauvegarder les meilleurs paramètres\n",
    "    joblib.dump(grid_search.best_params_, 'models/best_params.pkl')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    perform_grid_search()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import joblib\n",
    "\n",
    "def train_model():\n",
    "    # Charger les données d'entraînement\n",
    "    X_train = pd.read_csv('/home/ubuntu/Data_scientest/examen-dvc/data/processed_data/X_train_scaled.csv')\n",
    "    y_train = pd.read_csv('/home/ubuntu/Data_scientest/examen-dvc/data/processed_data/y_train.csv')\n",
    "    \n",
    "    # Charger les meilleurs paramètres\n",
    "    best_params = joblib.load('models/best_params.pkl')\n",
    "    \n",
    "    # Créer et entraîner le modèle\n",
    "    model = RandomForestRegressor(**best_params)\n",
    "    model.fit(X_train, y_train.values.ravel())\n",
    "    \n",
    "    # Sauvegarder le modèle entraîné\n",
    "    joblib.dump(model, 'models/trained_model.pkl')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "def evaluate_model():\n",
    "    # Charger les données de test\n",
    "    X_test = pd.read_csv('/home/ubuntu/Data_scientest/examen-dvc/data/processed_data/X_test_scaled.csv')\n",
    "    y_test = pd.read_csv('/home/ubuntu/Data_scientest/examen-dvc/data/processed_data/y_test.csv')\n",
    "    \n",
    "    # Charger le modèle entraîné\n",
    "    model = joblib.load('models/trained_model.pkl')\n",
    "    \n",
    "    # Faire des prédictions\n",
    "    predictions = model.predict(X_test)\n",
    "    \n",
    "    # Calculer les métriques\n",
    "    mse = mean_squared_error(y_test, predictions)\n",
    "    r2 = r2_score(y_test, predictions)\n",
    "    \n",
    "    # Sauvegarder les résultats\n",
    "    results = pd.DataFrame({'Actual': y_test.squeeze(), 'Predicted': predictions})\n",
    "    results.to_csv('data/predictions.csv', index=False)\n",
    "    \n",
    "    with open('metrics/scores.json', 'w') as f:\n",
    "        f.write(f'{{\"MSE\": {mse}, \"R2\": {r2}}}')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    evaluate_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####Autrze######################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import joblib\n",
    "\n",
    "def perform_grid_search():\n",
    "    X_train = pd.read_csv('/home/ubuntu/Data_scientest/examen-dvc/data/processed_data/X_train_scaled.csv')\n",
    "    y_train = pd.read_csv('/home/ubuntu/Data_scientest/examen-dvc/data/processed_data/y_train.csv')\n",
    "\n",
    "    model = RandomForestRegressor(random_state=42)\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [10, 20, 30],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4]\n",
    "    }\n",
    "\n",
    "    grid_search = GridSearchCV(model, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "    grid_search.fit(X_train, y_train.values.ravel())\n",
    "    \n",
    "    joblib.dump(grid_search.best_params_, '/home/ubuntu/Data_scientest/examen-dvc/models/best_params.pkl')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    perform_grid_search()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
